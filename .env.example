# Environment variables for Deep Scholar File Reconstruction
# Copy this file to .env and fill in your actual values

# Upload root (absolute path where UI drops files)
DEEP_SCHOLAR_UPLOAD_ROOT=/absolute/path/to/deep-agents-ui/public/uploads

# Persistent raw storage (immutable archive)
DEEP_SCHOLAR_PERSISTENT_RAW_ROOT=/absolute/path/to/data/raw

# Agent work root (artifacts like /user_intent.md)
DEEP_SCHOLAR_WORK_ROOT=/absolute/path/to/data/work

# Output root for reconstructed files
DEEP_SCHOLAR_OUTPUT_ROOT=/absolute/path/to/data/organized

# Persistent FAISS DB storage (optional)
DEEP_SCHOLAR_PERSISTENT_DB_ROOT=/absolute/path/to/data/db

# Embeddings endpoint for FAISS ingestion (optional)
DEEP_SCHOLAR_EMBEDDING_BASE_URL=http://127.0.0.1:8081/v1
DEEP_SCHOLAR_EMBEDDING_API_KEY=local-llama
DEEP_SCHOLAR_EMBEDDING_MODEL=qwen3-embed

# Optional local VLM (for OCR placeholders)
DEEP_SCHOLAR_VLM_BASE_URL=http://127.0.0.1:8081/v1
DEEP_SCHOLAR_VLM_API_KEY=local-llama
DEEP_SCHOLAR_VLM_MODEL=qwen3-vl

# Primary LLM selector: iflow | deepseek | llama
DEEP_SCHOLAR_LLM_PROVIDER=iflow

# IFlow (OpenAI-compatible) defaults
IFLOW_BASE_URL=https://apis.iflow.cn/v1
IFLOW_API_KEY=your_iflow_api_key_here
IFLOW_MODEL=qwen3-max
IFLOW_TEMPERATURE=0.2

# DeepSeek (OpenAI-compatible)
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE=0.2

# Local Llama (OpenAI-compatible endpoint such as Ollama/LM Studio)
LLAMA_BASE_URL=http://localhost:11434/v1
LLAMA_API_KEY=
LLAMA_MODEL=llama3
LLAMA_TEMPERATURE=0.2

# Anthropic API Key (optional, if you switch provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API Key (optional, if you switch provider)
OPENAI_API_KEY=your_openai_api_key_here

# LangSmith API Key (required for LangGraph local server)
# Get your key at: https://smith.langchain.com/settings
LANGSMITH_API_KEY=lsv2_pt_your_api_key_here
