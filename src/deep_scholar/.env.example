# Environment variables for Deep Scholar File Reconstruction
# Copy this file to .env and fill in your actual values

# Agent work root (all artifacts, inputs, outputs, and FAISS index live here)
DEEP_SCHOLAR_WORK_ROOT=/absolute/path/to/data/work
#
# The agent will create and use these subfolders under the work root:
# - uploads/ (incoming files)
# - raw/ (immutable archive copies)
# - output/ (reconstructed files)
# - db/ (FAISS indices)

# Embeddings endpoint for FAISS ingestion (optional)
DEEP_SCHOLAR_EMBEDDING_BASE_URL=http://127.0.0.1:8081/v1
DEEP_SCHOLAR_EMBEDDING_API_KEY=local-llama
DEEP_SCHOLAR_EMBEDDING_MODEL=qwen3-embed

# Optional local VLM (for OCR placeholders)
DEEP_SCHOLAR_VLM_BASE_URL=http://127.0.0.1:8081/v1
DEEP_SCHOLAR_VLM_API_KEY=local-llama
DEEP_SCHOLAR_VLM_MODEL=qwen3-vl

# Primary LLM selector: iflow | deepseek | llama
DEEP_SCHOLAR_LLM_PROVIDER=iflow

# IFlow (OpenAI-compatible) defaults
IFLOW_BASE_URL=https://apis.iflow.cn/v1
IFLOW_API_KEY=your_iflow_api_key_here
IFLOW_MODEL=qwen3-max
IFLOW_TEMPERATURE=0.2

# DeepSeek (OpenAI-compatible)
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE=0.2

# Local Llama (OpenAI-compatible endpoint such as Ollama/LM Studio)
LLAMA_BASE_URL=http://localhost:11434/v1
LLAMA_API_KEY=
LLAMA_MODEL=llama3
LLAMA_TEMPERATURE=0.2

# Anthropic API Key (optional, if you switch provider)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API Key (optional, if you switch provider)
OPENAI_API_KEY=your_openai_api_key_here

# LangSmith API Key (required for LangGraph local server)
# Get your key at: https://smith.langchain.com/settings
LANGSMITH_API_KEY=lsv2_pt_your_api_key_here
